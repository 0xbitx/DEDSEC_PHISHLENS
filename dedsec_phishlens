#! /usr/bin/python3

#coded by 0xbit

import os, sys, requests, socket, ssl, ssl, whois, joblib, time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import urlparse
from datetime import datetime
import pandas as pd
from tabulate import tabulate

class main_class:
    def __init__(self):
        self.green_dot = '[\033[92mâ—\033[0m]'
        self.green_question = '[\033[92m?\033[0m]'
        self.red_warning = '[\033[91m!\033[0m]'
        self.model = self.load_model()

    def load_model(self):
        try:
            print(f'\n\t  {self.green_dot} Loading [ \033[92mphishing_model_by_0xbit.model\033[0m ] Pre-trained Model...\n')
            return joblib.load("phishing_model_by_0xbit.model")
        except FileNotFoundError:
            print(f"\n\t  {self.red_warning} Error: Pre-trained model not found. Please train a model first.")
            exit()

    def extract_features(self, url):
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        path = parsed_url.path
        query = parsed_url.query

        features = {
            "qty_dot_url": url.count("."),
            "qty_hyphen_url": url.count("-"),
            "qty_underline_url": url.count("_"),
            "qty_slash_url": url.count("/"),
            "qty_questionmark_url": url.count("?"),
            "qty_equal_url": url.count("="),
            "qty_at_url": url.count("@"),
            "qty_and_url": url.count("&"),
            "qty_exclamation_url": url.count("!"),
            "qty_space_url": url.count(" "),
            "qty_tilde_url": url.count("~"),
            "qty_comma_url": url.count(","),
            "qty_plus_url": url.count("+"),
            "qty_asterisk_url": url.count("*"),
            "qty_hashtag_url": url.count("#"),
            "qty_dollar_url": url.count("$"),
            "qty_percent_url": url.count("%"),
            "qty_tld_url": len(domain.split(".")[-1]) if "." in domain else 0,
            "length_url": len(url),
            "qty_dot_domain": domain.count("."),
            "qty_hyphen_domain": domain.count("-"),
            "qty_underline_domain": domain.count("_"),
            "qty_slash_domain": domain.count("/"),
            "qty_questionmark_domain": domain.count("?"),
            "qty_equal_domain": domain.count("="),
            "qty_at_domain": domain.count("@"),
            "qty_and_domain": domain.count("&"),
            "qty_exclamation_domain": domain.count("!"),
            "qty_space_domain": domain.count(" "),
            "qty_tilde_domain": domain.count("~"),
            "qty_comma_domain": domain.count(","),
            "qty_plus_domain": domain.count("+"),
            "qty_asterisk_domain": domain.count("*"),
            "qty_hashtag_domain": domain.count("#"),
            "qty_dollar_domain": domain.count("$"),
            "qty_percent_domain": domain.count("%"),
            "qty_vowels_domain": sum(1 for char in domain if char.lower() in "aeiou"),
            "domain_length": len(domain),
            "domain_in_ip": 1 if any(part.isdigit() for part in domain.split(".")) else 0,
            "server_client_domain": 0, 
            "qty_dot_directory": path.count("."),
            "qty_hyphen_directory": path.count("-"),
            "qty_underline_directory": path.count("_"),
            "qty_slash_directory": path.count("/"),
            "qty_questionmark_directory": path.count("?"),
            "qty_equal_directory": path.count("="),
            "qty_at_directory": path.count("@"),
            "qty_and_directory": path.count("&"),
            "qty_exclamation_directory": path.count("!"),
            "qty_space_directory": path.count(" "),
            "qty_tilde_directory": path.count("~"),
            "qty_comma_directory": path.count(","),
            "qty_plus_directory": path.count("+"),
            "qty_asterisk_directory": path.count("*"),
            "qty_hashtag_directory": path.count("#"),
            "qty_dollar_directory": path.count("$"),
            "qty_percent_directory": path.count("%"),
            "directory_length": len(path),
            "qty_dot_file": path.split("/")[-1].count(".") if path else 0,
            "qty_hyphen_file": path.split("/")[-1].count("-") if path else 0,
            "qty_underline_file": path.split("/")[-1].count("_") if path else 0,
            "qty_slash_file": path.split("/")[-1].count("/") if path else 0,
            "qty_questionmark_file": path.split("/")[-1].count("?") if path else 0,
            "qty_equal_file": path.split("/")[-1].count("=") if path else 0,
            "qty_at_file": path.split("/")[-1].count("@") if path else 0,
            "qty_and_file": path.split("/")[-1].count("&") if path else 0,
            "qty_exclamation_file": path.split("/")[-1].count("!") if path else 0,
            "qty_space_file": path.split("/")[-1].count(" ") if path else 0,
            "qty_tilde_file": path.split("/")[-1].count("~") if path else 0,
            "qty_comma_file": path.split("/")[-1].count(",") if path else 0,
            "qty_plus_file": path.split("/")[-1].count("+") if path else 0,
            "qty_asterisk_file": path.split("/")[-1].count("*") if path else 0,
            "qty_hashtag_file": path.split("/")[-1].count("#") if path else 0,
            "qty_dollar_file": path.split("/")[-1].count("$") if path else 0,
            "qty_percent_file": path.split("/")[-1].count("%") if path else 0,
            "file_length": len(path.split("/")[-1]) if path else 0,
            "qty_dot_params": query.count("."),
            "qty_hyphen_params": query.count("-"),
            "qty_underline_params": query.count("_"),
            "qty_slash_params": query.count("/"),
            "qty_questionmark_params": query.count("?"),
            "qty_equal_params": query.count("="),
            "qty_at_params": query.count("@"),
            "qty_and_params": query.count("&"),
            "qty_exclamation_params": query.count("!"),
            "qty_space_params": query.count(" "),
            "qty_tilde_params": query.count("~"),
            "qty_comma_params": query.count(","),
            "qty_plus_params": query.count("+"),
            "qty_asterisk_params": query.count("*"),
            "qty_hashtag_params": query.count("#"),
            "qty_dollar_params": query.count("$"),
            "qty_percent_params": query.count("%"),
            "params_length": len(query),
            "tld_present_params": 1 if any(tld in query for tld in [".com", ".net", ".org"]) else 0,
            "qty_params": query.count("&") + 1 if query else 0,
            "email_in_url": 1 if "@" in url else 0,
            "time_response": 0, 
            "domain_spf": 0, 
            "asn_ip": 0,  
            "time_domain_activation": 0,  
            "time_domain_expiration": 0,  
            "qty_ip_resolved": 0, 
            "qty_nameservers": 0, 
            "qty_mx_servers": 0, 
            "ttl_hostname": 0, 
            "tls_ssl_certificate": 1 if url.startswith("https") else 0,
            "qty_redirects": 0, 
            "url_google_index": 0, 
            "domain_google_index": 0,  
            "url_shortened": 1 if any(service in url for service in ["bit.ly", "tinyurl", "goo.gl"]) else 0,
        }
        return pd.DataFrame([features])

    def get_info(self, domain):
        def format_date(date):
            if isinstance(date, list):
                return date[0].strftime("%Y-%m-%d %H:%M:%S") if date else "N/A"
            elif isinstance(date, datetime):
                return date.strftime("%Y-%m-%d %H:%M:%S")
            return "N/A"

        try:
            ip = socket.gethostbyname(domain)
        except Exception:
            ip = "Error resolving IP"

        geolocation = {}
        if ip and not ip.startswith("Error"):
            try:
                response = requests.get(f"http://ip-api.com/json/{ip}")
                if response.status_code == 200:
                    geolocation = response.json()
                else:
                    geolocation = {"error": "Geolocation API error"}
            except Exception:
                geolocation = {"error": f"Error retrieving geolocation"}

        domain_info = {}
        try:
            domain_info = whois.whois(domain)
        except Exception:
            domain_info = {"error": "Error retrieving domain info"}

        ssl_info = {}
        try:
            context = ssl.create_default_context()
            with socket.create_connection((domain, 443)) as sock:
                with context.wrap_socket(sock, server_hostname=domain) as ssock:
                    cert = ssock.getpeercert()
                    expiry_date = datetime.strptime(cert['notAfter'], "%b %d %H:%M:%S %Y %Z")
                    ssl_info = {
                        "issuer": dict(x[0] for x in cert['issuer']),
                        "expiry_date": expiry_date,
                        "is_valid": expiry_date > datetime.now()
                    }
        except Exception:
            ssl_info = {"error": f"Error retrieving SSL certificate info"}

        http_headers = {}
        try:
            response = requests.head(f"https://{domain}", allow_redirects=True)
            http_headers = response.headers
        except Exception:
            http_headers = {"error": f"Error retrieving HTTP headers"}

        ping_info = {}
        try:
            response = requests.get(f"https://{domain}")
            ping_info = {
                "status_code": response.status_code,
                "response_time": response.elapsed.total_seconds()
            }
        except Exception:
            ping_info = {"error": "Error pinging website"}

        table_data = [
            ["IP Address", ip],
            ["Domain", domain],
            ["Country", geolocation.get("country", geolocation.get("error", "N/A"))],
            ["Region", geolocation.get("regionName", geolocation.get("error", "N/A"))],
            ["City", geolocation.get("city", geolocation.get("error", "N/A"))],
            ["ISP", geolocation.get("isp", geolocation.get("error", "N/A"))],
            ["Organization", geolocation.get("org", geolocation.get("error", "N/A"))],
            ["AS Number", geolocation.get("as", geolocation.get("error", "N/A"))],
            ["Domain Creation Date", format_date(domain_info.get("creation_date", domain_info.get("error", "N/A")))],
            ["Domain Expiration Date", format_date(domain_info.get("expiration_date", domain_info.get("error", "N/A")))],
            ["SSL/TLS Issuer", ssl_info.get("issuer", {}).get("organization", ssl_info.get("error", "N/A")) if isinstance(ssl_info, dict) else ssl_info],
            ["SSL/TLS Expiry Date", ssl_info.get("expiry_date", ssl_info.get("error", "N/A")) if isinstance(ssl_info, dict) else ssl_info],
            ["SSL/TLS Valid", ssl_info.get("is_valid", ssl_info.get("error", "N/A")) if isinstance(ssl_info, dict) else ssl_info],
            ["Server", http_headers.get("Server", http_headers.get("error", "N/A"))],
            ["Content-Type", http_headers.get("Content-Type", http_headers.get("error", "N/A"))],
            ["Website Status Code", ping_info.get("status_code", ping_info.get("error", "N/A")) if isinstance(ping_info, dict) else ping_info],
            ["Response Time (seconds)", ping_info.get("response_time", ping_info.get("error", "N/A")) if isinstance(ping_info, dict) else ping_info]
        ]
        return table_data

    def check_domain_reputation(self, domain):
        with open('virustotal.api', 'r') as f:
            API_KEY = f.read().strip()
        url = f"https://www.virustotal.com/api/v3/domains/{domain}"
        headers = {"x-apikey": API_KEY}
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                reputation = data.get("data", {}).get("attributes", {}).get("last_analysis_stats", {})
                if reputation.get("malicious", 0) > 0:
                    return f"{self.red_warning} Warning: \033[91m {domain} is flagged as malicious by {reputation['malicious']} security vendors.\033[0m"
                return f"\033[92m{domain} has a good reputation.\033[0m"
            return f"{self.red_warning} Unable to check domain reputation."
        except requests.exceptions.RequestException:
            return f"{self.red_warning} Error: Unable to connect to the reputation service."

    def validate_ssl_certificate(self, domain):
        try:
            context = ssl.create_default_context()
            with socket.create_connection((domain, 443)) as sock:
                with context.wrap_socket(sock, server_hostname=domain) as ssock:
                    cert = ssock.getpeercert()
                    expiry_date = datetime.strptime(cert['notAfter'], "%b %d %H:%M:%S %Y %Z")
                    if expiry_date < datetime.now():
                        return f"{self.red_warning} Warning:\033[91m SSL certificate for {domain} has expired.\033[0m"
                    return f"\033[92mSSL certificate for {domain} is valid.\033[0m"
        except Exception:
            return f"{self.red_warning} Error: SSL certificate validation failed."

    def detect_suspicious_keywords(self, url):
        suspicious_keywords = [
            "login", "account", "verify", "password", "security", "update", "confirm",
            "bank", "paypal", "creditcard", "payment", "invoice", "transaction", "refund",
            "urgent", "immediate", "action required", "suspended", "locked", "terminated",
            "winner", "prize", "reward", "claim", "lottery", "bonus", "free", "offer",
            "click", "here", "link", "secure", "validate", "access", "unlock", "download",
            "phishing", "scam", "fraud", "hack", "malware", "virus", "spyware", "trojan",
            "amazon", "google", "facebook", "microsoft", "apple", "netflix", "ebay",
            "your account has been compromised", "verify your identity", "unusual login activity",
            "click now", "verify now", "limited time", "act now", "your account", "risk",
            "account update", "important update", "security alert", "confirm your identity",
            "password reset", "access your account", "log in", "claim your prize", "congratulations",
            "update required", "you have been selected", "validate your account", "final notice",
            "confirm now", "take action", "unauthorized activity", "sign in", "redeem now",
            "you are a winner", "download now", "urgent action required", "reset password",
            "limited offer", "exclusive deal", "verify account", "bank account", "payment declined",
            "upgrade required", "respond immediately"
        ]

        suspicious_keywords = list(set(suspicious_keywords))
        detected_keywords = [keyword for keyword in suspicious_keywords if keyword in url.lower()]
        if detected_keywords:
            return f"{self.red_warning} Warning:\033[91m Suspicious keywords detected: {', '.join(detected_keywords)}\033[0m"
        else:
            return "\033[92mNo suspicious keywords detected.\033[0m"
    
    def check_domain_age(self, domain):
        try:
            domain_info = whois.whois(domain)
            creation_date = domain_info.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
            domain_age = (datetime.now() - creation_date).days
            if domain_age < 365:
                return f"{self.red_warning} Warning:\033[91m Domain {domain} is relatively new ({domain_age} days old).\033[0m"
            return f"\033[92mDomain {domain} is {domain_age} days old (likely safe).\033[0m"
        except Exception as e:
            return f"{self.red_warning} Error: Unable to retrieve domain age. {str(e)}"

    def calculate_weighted_safety(self, domain_reputation, ssl_valid, suspicious_keywords, domain_age, ai_safe_probability):
        weights = {
            "domain_reputation": 0.2,
            "ssl_valid": 0.2,
            "suspicious_keywords": 0.2,
            "domain_age": 0.2,
            "ai_prediction": 0.2,
        }

        domain_reputation_score = 1 if "good reputation" in domain_reputation else 0
        ssl_valid_score = 1 if "valid" in ssl_valid else 0
        suspicious_keywords_score = 0 if "Warning" in suspicious_keywords else 1
        domain_age_score = 1 if "likely safe" in domain_age else 0

        weighted_score = (
            (domain_reputation_score * weights["domain_reputation"]) +
            (ssl_valid_score * weights["ssl_valid"]) +
            (suspicious_keywords_score * weights["suspicious_keywords"]) +
            (domain_age_score * weights["domain_age"]) +
            (ai_safe_probability * weights["ai_prediction"])
        )

        return weighted_score * 100
    
    def take_screenshot(self, url, domain):
        output_file_prefix = f"screenshot-{domain}-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        load_time = 1
        scroll_increment = 500
        num_scrolls = 1
        
        curdir = os.getcwd()
        
        if not os.path.exists(f'{curdir}/page-screenshots'):
            os.makedirs(f'{curdir}/page-screenshots')

        os.chdir(f'{curdir}/page-screenshots')
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        try:
            driver.get(url)
            driver.maximize_window()
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
            driver.execute_script("document.body.style.zoom='80%'")
            scroll_pause_time = load_time
            screenshot_count = 0
            for _ in range(num_scrolls):
                screenshot_filename = f"{output_file_prefix}-{screenshot_count}.png"
                driver.save_screenshot(screenshot_filename)
                os.system(f'feh {screenshot_filename} > /dev/null 2>&1 &')

                driver.execute_script(f"window.scrollBy(0, {scroll_increment});")
                time.sleep(scroll_pause_time)

                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height <= screenshot_count * scroll_increment:
                    break

                screenshot_count += 1
            
            return (f"\t {self.green_dot} Screenshot saved as {screenshot_filename}")
        except Exception as e:
            return (f"\t {self.red_dot} Error: {e}")
        finally:
            driver.quit()
            
    def analyze_url(self, url):
        print('\n'+'\n'.join(['\t ' + line for line in tabulate([[f'{self.green_dot} Analyzing URL: {url}']], tablefmt='fancy_grid').split('\n')]))

        domain = urlparse(url).netloc

        table_data = self.get_info(domain)
        print(f"\n\t  {self.green_dot} Domain Information:")
        print('\n'.join(['\t ' + line for line in tabulate(table_data, tablefmt='fancy_grid').split('\n')]))

        print(f"\n\t  {self.green_dot} Domain Reputation Check:")
        domain_reputation = self.check_domain_reputation(domain)
        print('\n'.join(['\t ' + line for line in tabulate([[domain_reputation]], tablefmt='fancy_grid').split('\n')]))

        print(f"\n\t  {self.green_dot} SSL Certificate Validation:")
        ssl_valid = self.validate_ssl_certificate(domain)
        print('\n'.join(['\t ' + line for line in tabulate([[ssl_valid]], tablefmt='fancy_grid').split('\n')]))

        print(f"\n\t  {self.green_dot} Suspicious Keyword Detection:")
        suspicious_keywords = self.detect_suspicious_keywords(url)
        print('\n'.join(['\t ' + line for line in tabulate([[suspicious_keywords]], tablefmt='fancy_grid').split('\n')]))

        print(f"\n\t  {self.green_dot} Domain Age Check:")
        domain_age = self.check_domain_age(domain)
        print('\n'.join(['\t ' + line for line in tabulate([[domain_age]], tablefmt='fancy_grid').split('\n')]))

        print(f"\n\t  {self.green_dot} AI-Based Phishing Detection:")
        features = self.extract_features(url)
        prediction = self.model.predict(features)
        prediction_proba = self.model.predict_proba(features)

        ai_safe_probability = prediction_proba[0][0]

        if prediction[0] == 1:
            print('\n'.join(['\t ' + line for line in tabulate([[f'\033[91mThis URL is likely a phishing attempt.\033[0m']], tablefmt='fancy_grid').split('\n')]))

        else:
            print('\n'.join(['\t ' + line for line in tabulate([['\033[92mThis URL appears to be safe.\033[0m']], tablefmt='fancy_grid').split('\n')]))
        
        print(f"\n\t  {self.green_dot} Visual Inspection: ")
        result = self.take_screenshot(url, domain)
        print('\n'.join(['\t ' + line for line in tabulate([[result]], tablefmt='fancy_grid').split('\n')]))

        weighted_safety_percentage = self.calculate_weighted_safety(
            domain_reputation, ssl_valid, suspicious_keywords, domain_age, ai_safe_probability
        )

        print(f"\n\t  {self.green_dot} Final Result:")
        print('\n'.join(['\t ' + line for line in tabulate([[f'{self.green_dot} Final Result: The website is {weighted_safety_percentage:.2f}% safe.']], tablefmt='fancy_grid').split('\n')]))
        print()
        
def banner():
    os.system('clear')
    banner = '''
    
                   .'|_.-
                 .'  '  /_
              .-"    -.   '>       
           .- -. -.    '. /    /|_  \033[92m P H I S H L E N S\033[0m
          .-.--.-.       ' >  /  /   DEDSED PHISHING DETECTION
         (o( o( o )       \\_."  <    coded by 0xbit
          '-'-''-'            ) <
        (       _.-'-.   ._\\\\.  _\\
         '----"/--.__.-) _-  \\|
               "V""    "V"

    '''
    print(banner)
    
if __name__ == "__main__":
    banner()
    detector = main_class()
    try:
        url = input(f"\t  {detector.green_question} Enter the URL to analyze: ")

        if not url.startswith("http://") and not url.startswith("https://") or url == "":
            print(f"\n\t  {detector.red_warning} Invalid URL format. Please enter a valid URL.")
            sys.exit(0)
        
        print(f"\n\t  {detector.green_dot} Checking URL: {url}")
        try:
            response = requests.get(url)
            if response.status_code == 200:
                pass
            else:
                sys.exit(f'\n\t  {detector.red_warning} Error: Unable to connect to the website.\n')
        except Exception:
            sys.exit(f'\n\t  {detector.red_warning} Error: Unable to connect to the website.\n')
            
        detector.analyze_url(url)

    except KeyboardInterrupt:
        os.system('clear')
        sys.exit(0)
